{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from typing import List, Tuple\n",
    "from deeprl.common.utils import get_gym_space_shape, net_gym_space_dims\n",
    "from torch.distributions import Categorical\n",
    "from deeprl.common.base import *\n",
    "from deeprl.common.utils import *\n",
    "from deeprl.common.buffers import *\n",
    "\n",
    "from deeprl.algos.a2c.a2c import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "from abc import ABC\n",
    "\n",
    "# TODO: Turn this into an abstract base class.\n",
    "class Memory:\n",
    "    \"\"\"\n",
    "    Memory provides storage and access for gym transitions.\n",
    "    This class will store the data as is and then convert to tensors as needed when sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, device):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = deque(maxlen=self.max_len)\n",
    "        self.device = device\n",
    "        self.keys = [\"states\", \"actions\", \"rewards\", \"dones\", \"next_states\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def to_torch(self, x):\n",
    "        arr_x = np.array(x).astype(np.float32)\n",
    "        out = torch.from_numpy(arr_x).to(self.device)\n",
    "        return out\n",
    "    \n",
    "    def reset_buffer(self):\n",
    "        \"\"\"Deletes contents of memory where the buffer lives\"\"\"\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def samples_to_batch(self, samples):\n",
    "        states, actions, rewards, dones, next_states = [to_torch(x, self.device) for x in zip(*samples)]\n",
    "        \n",
    "        batch = {\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"dones\": dones,\n",
    "            \"next_states\": next_states,\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def sample_batch(self, num_samples):\n",
    "        \"\"\"\n",
    "        returns an iterable(states, actions, rewards, dones, next_states)\n",
    "        \"\"\"\n",
    "        buffer_len = len(self.buffer)\n",
    "\n",
    "        # If there aren't enough samples then take what there is\n",
    "        if num_samples > buffer_len:\n",
    "            num_samples = buffer_len\n",
    "\n",
    "        samples = random.sample(self.buffer, num_samples)\n",
    "\n",
    "        batch = self.samples_to_batch(samples)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class OnPolicyMemory(Memory):\n",
    "    \"\"\"Version of memory where order matters.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, device):\n",
    "        super().__init__(max_len, device)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        \"\"\"Convert buffer to ordered stacks of different components instead of rows of transitions and convert these to float tensors.\n",
    "\n",
    "        Returns:\n",
    "            batch (dict[key: list(data)): Batch of experiences where keys correspond to each component recorded at a time step.\n",
    "        \"\"\"\n",
    "        # separate lists for each part of transition\n",
    "        states, actions, rewards, dones, next_states = [to_torch(data, self.device) for data in zip(*self.buffer)]\n",
    "        \n",
    "        batch = {\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"dones\": dones,\n",
    "            \"next_states\": next_states,\n",
    "        }\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "policy_layers = [\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": net_gym_space_dims(env.observation_space),\n",
    "        \"out_features\": 32}),\n",
    "    (nn.Tanh, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 32,\n",
    "        \"out_features\": 32}),\n",
    "    (nn.Tanh, {}),\n",
    "    (nn.Linear,{\"in_features\": 32, \"out_features\": net_gym_space_dims(env.action_space)}),\n",
    "]\n",
    "\n",
    "critic_layers = [\n",
    "    (nn.Linear, {\"in_features\": net_gym_space_dims(env.observation_space), \"out_features\": 32}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 32,\n",
    "        \"out_features\": 32}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\": 32, \"out_features\": 1}),\n",
    "]\n",
    "\n",
    "a2c_args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"env\": env,\n",
    "    \"step_lim\": 200,\n",
    "    \"policy\": CategoricalPolicy(policy_layers),\n",
    "    \"policy_optimiser\": optim.Adam,\n",
    "    \"policy_lr\": 0.001,\n",
    "    \"critic\": Network(critic_layers),\n",
    "    \"critic_lr\": 0.001,\n",
    "    \"critic_optimiser\": optim.Adam,\n",
    "    \"critic_criterion\": nn.MSELoss(),\n",
    "    \"device\": \"cpu\",\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"batch_size\": 100,\n",
    "    \"num_train_passes\": 2,\n",
    "    \"lam\": 0.2\n",
    "}\n",
    "agent = A2C(a2c_args)\n",
    "buff = OnPolicyMemory(100, \"cpu\")\n",
    "s = env.reset()\n",
    "for _ in range(200):\n",
    "    a = agent.choose_action(s)\n",
    "    assert env.action_space.contains(a)\n",
    "    s_, r, d, _ = env.step(a)\n",
    "    buff.store((s,a,r,d,s_))\n",
    "    if d:\n",
    "        s = env.reset()\n",
    "    else:\n",
    "        s = s_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[(array([-0.08962838, -0.1913191 ,  0.04213118,  0.28329834], dtype=float32), array([-0.09345476, -0.38701585,  0.04779715,  0.5889659 ], dtype=float32), array([-0.10119507, -0.19259465,  0.05957646,  0.31171417], dtype=float32), array([-0.10504697, -0.38851252,  0.06581075,  0.6225747 ], dtype=float32), array([-0.11281722, -0.5844887 ,  0.07826224,  0.935237  ], dtype=float32), array([-0.124507  , -0.3905046 ,  0.09696698,  0.66813713], dtype=float32), array([-0.13231708, -0.19685525,  0.11032972,  0.4074913 ], dtype=float32), array([-0.13625419, -0.39335454,  0.11847955,  0.73281926], dtype=float32), array([-0.14412127, -0.5898969 ,  0.13313593,  1.0603176 ], dtype=float32), array([-0.15591922, -0.78650665,  0.1543423 ,  1.3916489 ], dtype=float32)), (array(0, dtype=int64), array(1, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(1, dtype=int64), array(1, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(0, dtype=int64), array(1, dtype=int64)), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0), (False, False, False, False, False, False, False, False, False, False), (array([-0.09345476, -0.38701585,  0.04779715,  0.5889659 ], dtype=float32), array([-0.10119507, -0.19259465,  0.05957646,  0.31171417], dtype=float32), array([-0.10504697, -0.38851252,  0.06581075,  0.6225747 ], dtype=float32), array([-0.11281722, -0.5844887 ,  0.07826224,  0.935237  ], dtype=float32), array([-0.124507  , -0.3905046 ,  0.09696698,  0.66813713], dtype=float32), array([-0.13231708, -0.19685525,  0.11032972,  0.4074913 ], dtype=float32), array([-0.13625419, -0.39335454,  0.11847955,  0.73281926], dtype=float32), array([-0.14412127, -0.5898969 ,  0.13313593,  1.0603176 ], dtype=float32), array([-0.15591922, -0.78650665,  0.1543423 ,  1.3916489 ], dtype=float32), array([-0.17164935, -0.59360635,  0.18217526,  1.1509346 ], dtype=float32))]\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'states': tensor([[-0.0896, -0.1913,  0.0421,  0.2833],\n",
       "         [-0.0935, -0.3870,  0.0478,  0.5890],\n",
       "         [-0.1012, -0.1926,  0.0596,  0.3117],\n",
       "         [-0.1050, -0.3885,  0.0658,  0.6226],\n",
       "         [-0.1128, -0.5845,  0.0783,  0.9352],\n",
       "         [-0.1245, -0.3905,  0.0970,  0.6681],\n",
       "         [-0.1323, -0.1969,  0.1103,  0.4075],\n",
       "         [-0.1363, -0.3934,  0.1185,  0.7328],\n",
       "         [-0.1441, -0.5899,  0.1331,  1.0603],\n",
       "         [-0.1559, -0.7865,  0.1543,  1.3916]]),\n",
       " 'actions': tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 1.]),\n",
       " 'rewards': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'dones': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'next_states': tensor([[-0.0935, -0.3870,  0.0478,  0.5890],\n",
       "         [-0.1012, -0.1926,  0.0596,  0.3117],\n",
       "         [-0.1050, -0.3885,  0.0658,  0.6226],\n",
       "         [-0.1128, -0.5845,  0.0783,  0.9352],\n",
       "         [-0.1245, -0.3905,  0.0970,  0.6681],\n",
       "         [-0.1323, -0.1969,  0.1103,  0.4075],\n",
       "         [-0.1363, -0.3934,  0.1185,  0.7328],\n",
       "         [-0.1441, -0.5899,  0.1331,  1.0603],\n",
       "         [-0.1559, -0.7865,  0.1543,  1.3916],\n",
       "         [-0.1716, -0.5936,  0.1822,  1.1509]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.generate_experience(agent.batch_size)\n",
    "print(len(agent.buffer))\n",
    "out = agent.buffer.sample_batch()\n",
    "print(\"hi\")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = buff.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['states', 'actions', 'rewards', 'dones', 'next_states'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['next_states'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb0f9380c86a0fbbe7102c285f0fab3944b56c65416c68f89712163f80960c16"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
