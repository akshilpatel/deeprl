{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces.discrete import Discrete\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from gym import Space\n",
    "import torch.multiprocessing as mp\n",
    "from gym.vector import SyncVectorEnv, AsyncVectorEnv\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprl.common.utils import to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprl.common.utils import net_gym_space_dims, discount_cumsum, to_torch, compute_td_deltas, compute_gae_and_v_targets, normalise_adv\n",
    "from deeprl.algos.a2c.a2c import A2C\n",
    "from deeprl.common.base import Network, Policy, CategoricalPolicy, GaussianPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "envs = gym.vector.make(env_name)\n",
    "policy_layers = [\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": net_gym_space_dims(envs.single_observation_space),\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 20,\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,{\"in_features\": 20, \"out_features\": net_gym_space_dims(envs.single_action_space)}),\n",
    "]\n",
    "\n",
    "critic_layers = [\n",
    "    (nn.Linear, {\"in_features\": net_gym_space_dims(envs.single_observation_space), \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 20,\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\": 20, \"out_features\": 1}),\n",
    "]\n",
    "\n",
    "a2c_args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"env_name\": env_name,\n",
    "    \"step_lim\": 200,\n",
    "    \"policy\": CategoricalPolicy(policy_layers),\n",
    "    \"policy_optimiser\": optim.Adam,\n",
    "    \"policy_lr\": 0.002,\n",
    "    \"critic\": Network(critic_layers),\n",
    "    \"critic_lr\": 0.002,\n",
    "    \"critic_optimiser\": optim.Adam,\n",
    "    \"critic_criterion\": nn.MSELoss(),\n",
    "    \"device\": \"cpu\",\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"batch_size\": 250,\n",
    "    \"num_train_passes\": 1,\n",
    "    \"lam\": 0.95,\n",
    "    \"num_eval_episodes\": 15,\n",
    "    \"num_workers\": 4,\n",
    "    \"minibatch_size\": 10,\n",
    "    \"norm_adv\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(a2c_args)\n",
    "batch_size= 250\n",
    "num_workers = 4\n",
    "device=agent.device\n",
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "\n",
    "# Do this in the init? or in a train function\n",
    "def init_envs(env_name, num_envs, asynchronous):\n",
    "    envs = gym.vector.make(env_name, num_envs=num_envs, asynchronous=asynchronous)\n",
    "    envs.reset()\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout(agent, batch_size, num_workers, device, envs):\n",
    "    # Do this in the train function\n",
    "    states_batch = torch.zeros((batch_size, num_workers) + envs.single_observation_space.shape, dtype=torch.float).to(device)\n",
    "    actions_batch = torch.zeros((batch_size, num_workers) + envs.single_action_space.shape, dtype=torch.float).to(device)\n",
    "    rewards_batch = torch.zeros((batch_size, num_workers)).to(device)\n",
    "    dones_batch = torch.zeros((batch_size, num_workers)).to(device)\n",
    "    next_states_batch = torch.zeros((batch_size, num_workers) + envs.single_observation_space.shape).to(device)\n",
    "\n",
    "    # Put this into the rollout function\n",
    "    states = deepcopy(envs.observations)\n",
    "\n",
    "    for step in range(batch_size):\n",
    "        \n",
    "        actions = agent.choose_action(states)\n",
    "        next_states, rewards, dones, infos = envs.step(actions)\n",
    "        \n",
    "        states_batch[step] = to_torch(states, device)\n",
    "        actions_batch[step] = to_torch(actions, device)\n",
    "        rewards_batch[step] = to_torch(rewards, device)\n",
    "        dones_batch[step] = to_torch(dones, device)\n",
    "        next_states_batch[step] = to_torch(next_states, device)\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "    return states_batch, actions_batch, rewards_batch, dones_batch, next_states_batch\n",
    "\n",
    "\n",
    "def process_rollout(agent, rollout):\n",
    "    batches = [None for _ in range(num_workers)]\n",
    "    states_batch, actions_batch, rewards_batch, dones_batch, next_states_batch = rollout\n",
    "    for j in range(num_workers):\n",
    "        batch = {}\n",
    "        batch[\"states\"] = states_batch[:, j]\n",
    "        batch[\"actions\"] = actions_batch[:, j]\n",
    "        batch[\"rewards\"] = rewards_batch[:, j]\n",
    "        batch[\"dones\"] = dones_batch[:, j]\n",
    "        batch[\"next_states\"] = next_states_batch[:, j]\n",
    "        batch[\"advantages\"], batch[\"v_targets\"] = compute_gae_and_v_targets(agent.critic, batch, device, agent.gamma, agent.lam)\n",
    "        batches[j] = batch\n",
    "        # batch_reward = torch.reduce_sum(batch[\"rewards\"])\n",
    "        # batch_log = {\"rewards_sum\": batch_reward}\n",
    "\n",
    "    concat_batch = {k: torch.concat([b[k] for b in batches]) for k in batches[0].keys()}\n",
    "\n",
    "    return concat_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```assert len(concat_batch[\"states\"]) == batch_size * num_workers\n",
    "assert len(concat_batch[\"next_states\"]) == batch_size * num_workers\n",
    "assert concat_batch[\"states\"].dtype == torch.float32\n",
    "assert concat_batch[\"rewards\"].shape == (batch_size * num_workers,)\n",
    "assert concat_batch[\"dones\"].shape == (batch_size * num_workers,)\n",
    "assert concat_batch[\"actions\"].shape == (batch_size * num_workers,)\n",
    "assert (not concat_batch[\"advantages\"].requires_grad)````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function which shuffles and splits a batch (which has adv and v_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_split(batch, mb_size, shuffle=True):\n",
    "    \"\"\"Returns a list of minibatch dictionaries.\"\"\"\n",
    "    batch_size = len(batch[\"states\"])\n",
    "\n",
    "    if batch_size % mb_size != 0 or mb_size > batch_size:\n",
    "        raise ValueError(\"Minibatch size does not divide batch size.\")\n",
    "\n",
    "    batch_idc = np.arange(batch_size)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(batch_idc)\n",
    "    \n",
    "    mb_indices = [batch_idc[i: i+mb_size] for i in range(0, batch_size, mb_size)]\n",
    "    \n",
    "    minibatches = []\n",
    "\n",
    "    for mb_idc in mb_indices:\n",
    "        minibatch = {k: v[mb_idc] for k, v in batch.items()}\n",
    "        minibatches.append(minibatch)\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_from_batch(agent, num_passes, batch):\n",
    "    total_policy_loss = 0.\n",
    "    total_critic_loss = 0.\n",
    "    for _ in range(num_passes):\n",
    "        minibatches = minibatch_split(batch, agent.minibatch_size)\n",
    "        for minibatch in minibatches:\n",
    "            \n",
    "            if agent.norm_adv:\n",
    "                minibatch[\"advantages\"] = normalise_adv(minibatch[\"advantages\"])\n",
    "                \n",
    "            policy_loss = agent.update_policy(minibatch)\n",
    "            critic_loss = agent.update_critic(minibatch)\n",
    "\n",
    "            total_policy_loss += policy_loss[0]\n",
    "            total_critic_loss += critic_loss[0]\n",
    "\n",
    "    mean_policy_loss = total_policy_loss / num_passes / len(minibatches)\n",
    "    mean_critic_loss = total_critic_loss / num_passes / len(minibatches)\n",
    "\n",
    "    return mean_policy_loss, mean_critic_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300.0, 333.0, 228.0, 255.0, 259.0, 280.0, 184.0, 216.0, 260.0, 142.0]\n",
      "[500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "[9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 10.0, 9.0, 9.0, 9.0]\n",
      "[69.0, 57.0, 87.0, 79.0, 44.0, 77.0, 70.0, 72.0, 46.0, 83.0]\n",
      "[9.0, 10.0, 10.0, 9.0, 9.0, 8.0, 9.0, 10.0, 10.0, 8.0]\n"
     ]
    }
   ],
   "source": [
    "num_passes=4\n",
    "envs = init_envs(env_name, num_workers, False)\n",
    "r = []\n",
    "num_epochs = 5\n",
    "for e in range(num_epochs):\n",
    "    rollout = collect_rollout(agent, batch_size, num_workers, device, envs)\n",
    "    batch = process_rollout(agent, rollout)\n",
    "    p_loss, c_loss = update_from_batch(agent, num_passes, batch)\n",
    "    print(agent.run_eval())\n",
    "    \n",
    "# Why are the rewards so big?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e01f14186573fd8eb9a133e0d3241cc6898a0fbe7583218b264114aff85a110d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rl_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
