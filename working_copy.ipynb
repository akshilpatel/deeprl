{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "### DQN on cartpole \n",
    "- [x] Read about DQN and plan classes\n",
    "- [ ] Test for dtypes, shapes, correct gradient changes, everything is a tensor, tests for making sure no_grad is on in the right points\n",
    "- [ ] Assert optimiser has zero grad just before it calculates the gradients\n",
    "- [ ] Make sure every input to a network is a tensor, and every input to a gym env is of right type and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions/cheat sheet\n",
    "- When do i stop the gradients? Only have gradients when computing the loss for an update. \n",
    "    - Compute gradients for calculating things which have used the weights which need updating. \n",
    "    - I don't want requires_grad for environment outputs, only weights/params in the learnable nets\n",
    "- Do the dimensions of inputs to network have to include batch size, even if it is always 1?  I think so. Whenever an input goes into a network basically.\n",
    "- How do i pass a gradient through a distribution\n",
    "    - Using rdist\n",
    "- Convert everything that needs to go into a network to a tensor, float, device, has to have 0th dim as batch size when input for a net\n",
    "    - Dtype is float for everything since tensors are only used to go in the network. \n",
    "    - So the dtype only matters in the networks, and float is safe for passing through networks.\n",
    "- Use `.gather()` for getting the Q values as it is differentiable \n",
    "- Do i need to do a deepcopy of all transitions when sampling from buffer? \n",
    "    - No because I convert them to a tensor anyway which produces a new memory and reference\n",
    "- Only need to transfer to device when I when I create a new tensor or model that you want the parameters to be on cuda. \n",
    "    - So model has params (weights) so it is on cuda, most loss functions are not on cuda, any inputs and outputs to something on cuda are on cuda, so states etc, inputs etc have to be on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from typing import List, Tuple\n",
    "from deeprl.common.utils import get_gym_space_shape, net_gym_space_dims\n",
    "from torch.distributions import Categorical\n",
    "from deeprl.common.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<CartPoleEnv<CartPole-v1>>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, design: List):\n",
    "        '''\n",
    "        Args:\n",
    "            design List(Tuple): Each Tuple is of form (nn.Layer, {param_name: param_values})\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "\n",
    "        for i in range(len(design)):\n",
    "            layer_type, params = design[i]\n",
    "            self.layers.append(layer_type(**params))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert type(x) == torch.Tensor\n",
    "        assert x.dim() > 1\n",
    "        assert x.dtype in [float, torch.float64, torch.float32]\n",
    "        \n",
    "        out = self.net(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVNAME = 'CartPole-v1'\n",
    "env = gym.make(ENVNAME)\n",
    "# env2 = gym.make('BipedalWalker-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_gym_space_dims(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = [\n",
    "    (nn.Linear, {\"in_features\":net_gym_space_dims(env.observation_space), \"out_features\":128}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\":128, \"out_features\": 64}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\":64, \"out_features\": net_gym_space_dims(env.action_space)})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalPolicy(Network):\n",
    "    def __init__(self, arch):\n",
    "        super().__init__(arch)\n",
    "\n",
    "    def sample(self, state, action=None):\n",
    "        params = self.forward(state)  # gives unnormalised logits\n",
    "        assert type(params) == torch.Tensor\n",
    "        assert torch.is_floating_point(params)\n",
    "\n",
    "        prob_dist = Categorical(logits=params)  # use logits if unnormalised,\n",
    "\n",
    "        if action is None:\n",
    "            action = prob_dist.sample()\n",
    "\n",
    "        log_prob = prob_dist.log_prob(action)\n",
    "        entropy = prob_dist.entropy()\n",
    "\n",
    "        return action, log_prob, entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshil\\AppData\\Local\\Temp\\ipykernel_13920\\801187613.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  state = torch.tensor([state], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.tensor([state], dtype=torch.float)\n",
    "policy = CategoricalPolicy(des)\n",
    "a, lp, ent = policy.sample(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.contains(a.cpu().detach().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(lp) == torch.Tensor\n",
    "assert lp.dtype == torch.float\n",
    "assert lp.requires_grad\n",
    "assert lp.shape == (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(ent) == torch.Tensor\n",
    "assert ent.dtype == torch.float\n",
    "assert ent.requires_grad\n",
    "assert ent.shape == (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self, args):     \n",
    "        self.device = args[\"device\"]\n",
    "        self.gamma = args[\"gamma\"]\n",
    "        self.env = args[\"env\"]\n",
    "        self.step_lim = args[\"step_lim\"]\n",
    "\n",
    "        # Critic\n",
    "        self.critic = args[\"critic\"].to(self.device)\n",
    "        self.critic_lr = args[\"critic_lr\"]\n",
    "        self.critic_optimiser = args[\"critic_optimiser\"](\n",
    "            self.critic.parameters(), self.critic_lr\n",
    "        )\n",
    "        self.critic_criterion = args[\"critic_criterion\"]\n",
    "\n",
    "        # Policy\n",
    "        self.policy = args[\"policy\"].to(self.device)\n",
    "        self.policy_lr = args[\"policy_lr\"]\n",
    "        self.policy_optimiser = args[\"policy_optimiser\"](\n",
    "            self.policy.parameters(), self.policy_lr\n",
    "        )\n",
    "        self.entropy_coef = args[\"entropy_coef\"]\n",
    "        # self.adv_n = 0.\n",
    "        # self.adv_mean = 0.\n",
    "\n",
    "\n",
    "    def train(self, num_episodes:int, render=False, verbose=True) -> np.array:\n",
    "        \"\"\"This is a wrapper method around the `run_episode` method. We use `train` mehod for running experiments. \n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): The number of episodes for which to run training.\n",
    "            render (bool, optional): Flag used to render episodes to watch training. Defaults to False.\n",
    "            verbose (bool, optional): Used to decide if we should . Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Array of shape (num_episodes,) which contains the episodic rewards returned by the `run_episode` method. \n",
    "        \"\"\"\n",
    "\n",
    "        total_rewards = np.zeros(num_episodes)\n",
    "        for i in range(num_episodes):\n",
    "            total_rewards[i] = self.run_episode(render=render) \n",
    "\n",
    "            if i%10==0 and verbose:\n",
    "                print(\"Episode {}, Reward {}\".format(i, total_rewards[i]))\n",
    "        return total_rewards\n",
    "\n",
    "    def run_episode(self, render=False):\n",
    "        \"\"\"Functionality for running an episode with updating.\n",
    "\n",
    "        Args:\n",
    "            render (bool, optional): Flag used to render episodes to watch training. Defaults to False. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        state = self.env.reset()\n",
    "        episodic_reward = 0.\n",
    "        step_counter = 0\n",
    "        while step_counter < self.step_lim:\n",
    "            \n",
    "            action, log_prob, entropy = self.choose_action(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            episodic_reward += reward \n",
    "            step_counter +=1\n",
    "            \n",
    "            if render: self.env.render()\n",
    "            \n",
    "            losses = self.update(state, reward, next_state, done, log_prob, entropy)\n",
    "            \n",
    "            if done: \n",
    "                # print(losses)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return episodic_reward\n",
    "\n",
    "    def update(self, state, reward, next_state, done:bool, log_prob, entropy):\n",
    "        \"\"\"One step update which:\n",
    "        1) Converts the experience to tensors and puts on the correct device\n",
    "        2) Calls `.compute_adv_and_td_target` method to compute the advantage and td_target (without gradients) \n",
    "        3) Calls the `update_policy` and `update_critic` methods using the advantage and td_target to update both policy and critic.\n",
    "\n",
    "        Args:\n",
    "            state (numpy array): State from which the agent starts the transition\n",
    "            reward (float or int): One step reward for the transition\n",
    "            next_state (numpy array): Next state after taking the action.\n",
    "            done (bool): True iff the next_state was terminal. \n",
    "            log_prob (torch.FloatTensor): shape=(1,1)\n",
    "            entropy (torch.FloatTensor): shape=(1,1)\n",
    "\n",
    "        Returns:\n",
    "            Tuple(float, float): (Policy Loss, Critic Loss) computed for the transition.\n",
    "        \"\"\"\n",
    "\n",
    "        state = torch.tensor([state], dtype=torch.float, device=self.device)\n",
    "        next_state = torch.tensor([next_state], dtype=torch.float, device=self.device)\n",
    "        # done = torch.tensor(done, dtype=torch.int, device=self.device) # are these two lines needed since they don't go through a model.\n",
    "        # reward = torch.tensor(reward, dtype=torch.float, device=self.device)\n",
    "\n",
    "        # print(state, next_state, done, reward)\n",
    "\n",
    "        # These are both targets so no need to track grads\n",
    "        # with torch.no_grad():\n",
    "        adv, td_target = self.compute_adv_and_td_target(state, reward, next_state, done)\n",
    "\n",
    "        assert not adv.requires_grad\n",
    "        assert not td_target.requires_grad\n",
    "\n",
    "        policy_loss = self.update_policy(log_prob, entropy, adv)\n",
    "        critic_loss = self.update_critic(td_target, state)\n",
    "\n",
    "        return policy_loss, critic_loss\n",
    "\n",
    "    def update_policy(self, log_prob, entropy, adv):\n",
    "        \n",
    "        assert not adv.requires_grad\n",
    "        assert log_prob.requires_grad\n",
    "        # print(log_prob)\n",
    "        # print(adv)\n",
    "        \n",
    "        policy_loss = -(log_prob * adv) #- (entropy * self.entropy_coef)\n",
    "        # print(policy_loss, policy_loss.shape)\n",
    "        policy_loss = policy_loss.squeeze()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(self.policy.parameters(), 0.2)\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        return policy_loss.item()\n",
    "\n",
    "    def update_critic(self, td_target, state):\n",
    "        # Compute the current state value with grads\n",
    "        # Compute the \n",
    "        \n",
    "        assert not td_target.requires_grad\n",
    "\n",
    "        current_state_val = self.critic(state)\n",
    "        assert td_target.shape == current_state_val.shape\n",
    "        assert current_state_val.requires_grad\n",
    "        \n",
    "        critic_loss = self.critic_criterion(current_state_val, td_target)\n",
    "        assert critic_loss.shape == ()\n",
    "        \n",
    "        self.critic_optimiser.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(self.critic.parameters(), 0.2)\n",
    "        self.critic_optimiser.step()\n",
    "\n",
    "        return critic_loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_adv_and_td_target(self, state, reward, next_state, done):\n",
    "        \n",
    "\n",
    "        td_target = reward + self.gamma * self.critic(next_state) * (1-done)\n",
    "        adv = td_target - self.critic(state)\n",
    "        \n",
    "        assert not td_target.requires_grad\n",
    "        assert not adv.requires_grad\n",
    "        assert adv.shape == (1,1)\n",
    "        assert td_target.shape == (1,1)\n",
    "        \n",
    "\n",
    "        \n",
    "        return adv, td_target\n",
    "\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # Convert state to torch tensor\n",
    "        # Pass state to policy net\n",
    "        # Convert action to numpy \n",
    "        state = torch.tensor([state], dtype=torch.float, device=self.device)\n",
    "        action, log_prob, entropy = self.policy.sample(state)\n",
    "\n",
    "        action = action.cpu().detach().numpy().squeeze()\n",
    "        \n",
    "        assert self.env.action_space.contains(action)\n",
    "\n",
    "        return action, log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "policy_layers = [\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": net_gym_space_dims(env.observation_space),\n",
    "        \"out_features\": 32}),\n",
    "    (nn.Tanh, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 32,\n",
    "        \"out_features\": 32}),\n",
    "    (nn.Tanh, {}),\n",
    "    (nn.Linear,{\"in_features\": 32, \"out_features\": net_gym_space_dims(env.action_space)}),\n",
    "]\n",
    "\n",
    "critic_layers = [\n",
    "    (nn.Linear, {\"in_features\": net_gym_space_dims(env.observation_space), \"out_features\": 32}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 32,\n",
    "        \"out_features\": 32}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\": 32, \"out_features\": 1}),\n",
    "]\n",
    "\n",
    "a2c_args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"env\": env,\n",
    "    \"step_lim\": 200,\n",
    "    \"policy\": CategoricalPolicy(policy_layers),\n",
    "    \"policy_optimiser\": optim.Adam,\n",
    "    \"policy_lr\": 0.001,\n",
    "    \"critic\": Network(critic_layers),\n",
    "    \"critic_lr\": 0.001,\n",
    "    \"critic_optimiser\": optim.Adam,\n",
    "    \"critic_criterion\": nn.MSELoss(),\n",
    "    \"device\": \"cpu\",\n",
    "    \"entropy_coef\": 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(a2c_args)\n",
    "s = env.reset()\n",
    "for _ in range(200):\n",
    "    a, lp, ent = agent.choose_action(s)\n",
    "    assert env.action_space.contains(a)\n",
    "    assert lp.requires_grad\n",
    "    assert ent.requires_grad\n",
    "    s_, r, d, _ = env.step(a)\n",
    "    if d:\n",
    "        s = env.reset()\n",
    "    else:\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for agent number 0\n",
      "Episode 0, Reward 20.0\n",
      "Episode 10, Reward 92.0\n",
      "Episode 20, Reward 50.0\n",
      "Episode 30, Reward 17.0\n",
      "Episode 40, Reward 119.0\n",
      "Episode 50, Reward 121.0\n",
      "Episode 60, Reward 200.0\n",
      "Episode 70, Reward 200.0\n",
      "Episode 80, Reward 30.0\n",
      "Episode 90, Reward 30.0\n",
      "Episode 100, Reward 26.0\n",
      "Episode 110, Reward 23.0\n",
      "Episode 120, Reward 9.0\n",
      "Episode 130, Reward 9.0\n",
      "Episode 140, Reward 8.0\n",
      "Episode 150, Reward 9.0\n",
      "Episode 160, Reward 10.0\n",
      "Episode 170, Reward 8.0\n",
      "Episode 180, Reward 9.0\n",
      "Episode 190, Reward 10.0\n",
      "Running training for agent number 1\n",
      "Episode 0, Reward 10.0\n",
      "Episode 10, Reward 10.0\n",
      "Episode 20, Reward 10.0\n",
      "Episode 30, Reward 8.0\n",
      "Episode 40, Reward 10.0\n",
      "Episode 50, Reward 11.0\n",
      "Episode 60, Reward 9.0\n",
      "Episode 70, Reward 9.0\n",
      "Episode 80, Reward 9.0\n",
      "Episode 90, Reward 8.0\n",
      "Episode 100, Reward 10.0\n",
      "Episode 110, Reward 9.0\n",
      "Episode 120, Reward 10.0\n",
      "Episode 130, Reward 9.0\n",
      "Episode 140, Reward 11.0\n",
      "Episode 150, Reward 9.0\n",
      "Episode 160, Reward 10.0\n",
      "Episode 170, Reward 9.0\n",
      "Episode 180, Reward 8.0\n",
      "Episode 190, Reward 9.0\n",
      "Running training for agent number 2\n",
      "Episode 0, Reward 10.0\n",
      "Episode 10, Reward 9.0\n",
      "Episode 20, Reward 9.0\n",
      "Episode 30, Reward 10.0\n",
      "Episode 40, Reward 9.0\n",
      "Episode 50, Reward 10.0\n",
      "Episode 60, Reward 10.0\n",
      "Episode 70, Reward 9.0\n",
      "Episode 80, Reward 10.0\n",
      "Episode 90, Reward 9.0\n",
      "Episode 100, Reward 10.0\n",
      "Episode 110, Reward 10.0\n",
      "Episode 120, Reward 10.0\n",
      "Episode 130, Reward 9.0\n",
      "Episode 140, Reward 9.0\n",
      "Episode 150, Reward 8.0\n",
      "Episode 160, Reward 9.0\n",
      "Episode 170, Reward 9.0\n",
      "Episode 180, Reward 8.0\n",
      "Episode 190, Reward 10.0\n",
      "Running training for agent number 3\n",
      "Episode 0, Reward 10.0\n",
      "Episode 10, Reward 10.0\n",
      "Episode 20, Reward 9.0\n",
      "Episode 30, Reward 8.0\n",
      "Episode 40, Reward 9.0\n",
      "Episode 50, Reward 8.0\n",
      "Episode 60, Reward 10.0\n",
      "Episode 70, Reward 10.0\n",
      "Episode 80, Reward 9.0\n",
      "Episode 90, Reward 8.0\n",
      "Episode 100, Reward 10.0\n",
      "Episode 110, Reward 10.0\n",
      "Episode 120, Reward 10.0\n",
      "Episode 130, Reward 10.0\n",
      "Episode 140, Reward 9.0\n",
      "Episode 150, Reward 8.0\n",
      "Episode 160, Reward 9.0\n",
      "Episode 170, Reward 9.0\n",
      "Episode 180, Reward 10.0\n",
      "Episode 190, Reward 9.0\n",
      "Running training for agent number 4\n",
      "Episode 0, Reward 10.0\n",
      "Episode 10, Reward 9.0\n",
      "Episode 20, Reward 10.0\n",
      "Episode 30, Reward 11.0\n",
      "Episode 40, Reward 8.0\n",
      "Episode 50, Reward 8.0\n",
      "Episode 60, Reward 8.0\n",
      "Episode 70, Reward 10.0\n",
      "Episode 80, Reward 8.0\n",
      "Episode 90, Reward 9.0\n",
      "Episode 100, Reward 10.0\n",
      "Episode 110, Reward 9.0\n",
      "Episode 120, Reward 10.0\n",
      "Episode 130, Reward 10.0\n",
      "Episode 140, Reward 9.0\n",
      "Episode 150, Reward 9.0\n",
      "Episode 160, Reward 9.0\n",
      "Episode 170, Reward 10.0\n",
      "Episode 180, Reward 9.0\n",
      "Episode 190, Reward 9.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_agents = 5\n",
    "num_epi = 200\n",
    "r = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    print(\"Running training for agent number {}\".format(i))\n",
    "    agent = A2C(a2c_args)\n",
    "        \n",
    "    # random.seed(i)\n",
    "    # np.random.seed(i)\n",
    "    # torch.manual_seed(i)\n",
    "    # env.seed(i)\n",
    "\n",
    "    r.append(agent.train(num_epi))\n",
    "\n",
    "out = np.array(r).mean(0)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.title('A2C on cartpole')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episodic Reward')\n",
    "plt.plot(out, label='rewards')\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('./data/a2c_cartpole.PNG')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
