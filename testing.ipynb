{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "### DQN on cartpole \n",
    "- [x] Read about DQN and plan classes\n",
    "- [ ] Test for dtypes, shapes, correct gradient changes, everything is a tensor, tests for making sure no_grad is on in the right points\n",
    "- [ ] Assert optimiser has zero grad just before it calculates the gradients\n",
    "- [ ] Make sure every input to a network is a tensor, and every input to a gym env is of right type and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions/cheat sheet\n",
    "- When do i stop the gradients? Only have gradients when computing the loss for an update. \n",
    "    - Compute gradients for calculating things which have used the weights which need updating. \n",
    "    - I don't want requires_grad for environment outputs, only weights/params in the learnable nets\n",
    "- Do the dimensions of inputs to network have to include batch size, even if it is always 1?  I think so. Whenever an input goes into a network basically.\n",
    "- How do i pass a gradient through a distribution\n",
    "    - Using rdist\n",
    "- Convert everything that needs to go into a network to a tensor, float, device, has to have 0th dim as batch size when input for a net\n",
    "    - Dtype is float for everything since tensors are only used to go in the network. \n",
    "    - So the dtype only matters in the networks, and float is safe for passing through networks.\n",
    "- Use `.gather()` for getting the Q values as it is differentiable \n",
    "- Do i need to do a deepcopy of all transitions when sampling from buffer? \n",
    "    - No because I convert them to a tensor anyway which produces a new memory and reference\n",
    "- Only need to transfer to device when I when I create a new tensor or model that you want the parameters to be on cuda. \n",
    "    - So model has params (weights) so it is on cuda, most loss functions are not on cuda, any inputs and outputs to something on cuda are on cuda, so states etc, inputs etc have to be on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces.discrete import Discrete\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from gym import Space\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshil\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "ENVNAME = 'CartPole-v1'\n",
    "env = gym.make(ENVNAME)\n",
    "env2 = gym.make('BipedalWalker-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gym_space_shape(space):\n",
    "    if isinstance(space, Box):\n",
    "        return space.shape[0]\n",
    "    elif isinstance(space, Discrete):\n",
    "        return space.n\n",
    "    else:\n",
    "        raise TypeError(\"You haven't input a valid gym space. Here is your input: {}\".format(space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_torch_dtype(np_dtype):\n",
    "    numpy_to_torch_dtype = {\n",
    "        np.bool_       : torch.bool,\n",
    "        np.uint8      : torch.uint8,\n",
    "        np.int8       : torch.int8,\n",
    "        np.int16      : torch.int16,\n",
    "        np.int32      : torch.int32,\n",
    "        np.int64      : torch.int64,\n",
    "        np.float16    : torch.float16,\n",
    "        np.float32    : torch.float32,\n",
    "        np.float64    : torch.float64,\n",
    "        np.complex64  : torch.complex64,\n",
    "        np.complex128 : torch.complex128\n",
    "    }\n",
    "    return numpy_to_torch_dtype[np_dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, design: List):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        # Define each layer according to arch.\n",
    "        for i in range(len(design)):\n",
    "            layer_type, params = design[i]\n",
    "            self.layers.append(layer_type(**params))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert type(x) == torch.Tensor\n",
    "        assert x.dim() > 1\n",
    "        assert x.dtype in [float, torch.float, torch.float64]\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"\n",
    "    Memeory does not convert what is given from the environment. It only provides storage and access.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, device):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = self._reset_buffer()\n",
    "        self.device = device\n",
    "#         self.state_dtype = numpy_to_torch_dtype(env.observation_space.dtype)\n",
    "#         self.action_dtype = numpy_to_torch_dtype(env.action_space.dtype)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def _reset_buffer(self):\n",
    "        return deque(maxlen=self.max_len)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        returns an iterable(states, actions, rewards, dones, next_states)\n",
    "        \"\"\"\n",
    "        buffer_len = len(self.buffer)\n",
    "        # If there aren't enough samples then take the min\n",
    "        samples = random.sample(self.buffer, min([num_samples, buffer_len]))\n",
    "        \n",
    "        # separate lists for each part of transition\n",
    "        states, actions, rewards, dones, next_states = zip(*samples)\n",
    "        \n",
    "        # Convert to tensors and put on device\n",
    "        # calculate targets with target net \n",
    "        states = torch.tensor(states, dtype=torch.float, device=self.device) # shape:(mb_size, state_dim) using torch.float downgrades to float32 which i don't think matters...\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float, device=self.device) #shape: (mb_size, state_dim)\n",
    "        actions = torch.tensor(actions, dtype=torch.float, device=self.device) # shape:(mb_size, action_dim) used for indexing only \n",
    "        \n",
    "        rewards = torch.tensor([rewards], dtype=torch.float, device=self.device) # mb_size,added to output\n",
    "        dones = torch.tensor(dones, dtype=torch.int, device=self.device) # (mb_size, 1)\n",
    "        \n",
    "        return states, actions, rewards, dones, next_states\n",
    "            \n",
    "            \n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory_sample():\n",
    "    m = Memory(100, 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ENVNAME = 'CartPole-v1'\n",
    "    env = gym.make(ENVNAME)\n",
    "    \n",
    "    s = env.reset()\n",
    "    for i in range(50):\n",
    "        a = env.action_space.sample()\n",
    "        s_, r, d, _ = env.step(a)\n",
    "        t = [s, a, r, d, s_]\n",
    "        m.store(t)\n",
    "        if d: \n",
    "            s = env.reset()\n",
    "        \n",
    "    states, actions, rewards, dones, next_states = m.sample(120)\n",
    "    assert len(m) == 50\n",
    "    assert states.shape == (50, 4)\n",
    "test_memory_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVNAME = 'CartPole-v1'\n",
    "env = gym.make(ENVNAME)\n",
    "m = Memory(100, 'cuda')\n",
    "s = env.reset()\n",
    "for i in range(50):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, d, _ = env.step(a)\n",
    "    t = [s, a, r, d, s_]\n",
    "    m.store(t)\n",
    "    if d: \n",
    "        s = env.reset()\n",
    "\n",
    "states, actions, rewards, dones, next_states = m.sample(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = get_gym_space_shape(env.observation_space)\n",
    "output_dim = get_gym_space_shape(env.action_space)\n",
    "net_layers = [(nn.Linear, {\"in_features\": input_dim, \"out_features\": 16}), \n",
    "              (nn.ReLU, {}),\n",
    "              (nn.Linear, {\"in_features\": 16, \"out_features\": 4}),\n",
    "              (nn.ReLU, {}),\n",
    "              (nn.Linear, {\"in_features\": 4, \"out_features\": output_dim})]\n",
    "cartpole_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_args = {'gamma'          : 0.9,\n",
    "            'epsilon'        : 0.3,\n",
    "            'eps_decay_rate' : 0.999,\n",
    "            'env'            : cartpole_env,\n",
    "            'step_lim'       : 200,\n",
    "            'mb_size'        : 32,\n",
    "            'net_design'     : net_layers,\n",
    "            'optimiser'      : optim.Adam,\n",
    "            'lr'             : 0.0001,\n",
    "            'polyak_w'       : 1.,\n",
    "            'memory_max_len' : 50000,\n",
    "            'device'         : 'cpu', #torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "            'criterion'      : nn.MSELoss(),\n",
    "            'target_update_freq' : 30            \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, kwargs):\n",
    "        # algo params\n",
    "        self.gamma = kwargs['gamma']\n",
    "        self.lr = kwargs['lr']\n",
    "        self.epsilon = kwargs['epsilon']\n",
    "        self.eps_decay_rate = kwargs['eps_decay_rate']\n",
    "        self.mb_size = kwargs['mb_size']\n",
    "        self.polyak_w = kwargs['polyak_w']\n",
    "        \n",
    "        # env params\n",
    "        self.step_lim = kwargs['step_lim']\n",
    "        self.env = kwargs['env']\n",
    "        \n",
    "        self.device = kwargs['device']\n",
    "        self.memory = Memory(kwargs['memory_max_len'], self.device)\n",
    "        self.q = Network(kwargs['net_design']) # use for action selection (nograd) and learning(grad)\n",
    "        self.q.to(self.device)\n",
    "        self.target_q = deepcopy(self.q)\n",
    "        self.target_q.eval()\n",
    "        self.target_update_freq = kwargs['target_update_freq']\n",
    "        self._target_timer = 0\n",
    "        self.criterion = kwargs['criterion']\n",
    "        self.optimiser = kwargs['optimiser'](self.q.parameters(), self.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([state], dtype=torch.float, device=self.device) # now passing through network, so should be float really\n",
    "            \n",
    "            self.q.eval()\n",
    "            # convert to numpy so you can do random choice of max vals\n",
    "            q_vals = self.q(state).cpu().detach().numpy() # (1, num_ac) do i need to detach given that i have already no_grad?\n",
    "            self.q.train()\n",
    "    \n",
    "            greedy_actions = np.where(q_vals == q_vals.max())[0] \n",
    "            action = np.random.choice(greedy_actions)\n",
    "        \n",
    "        assert self.env.action_space.contains(action)\n",
    "        return action\n",
    "                 \n",
    "    def _q_update(self):\n",
    "        states, actions, rewards, dones, next_states = self.memory.sample(self.mb_size)\n",
    "        rewards = rewards.squeeze(0)\n",
    "        \n",
    "        # Online Q net - track this for backprop\n",
    "        state_q_vals = self.q(states)\n",
    "        \n",
    "        # actions are converted to ints and used as indices, then the whole thing is squeezed to match targets shape for loss\n",
    "        q_preds = torch.gather(state_q_vals, -1, actions.long().unsqueeze(-1))\n",
    "        q_preds.squeeze_(-1)\n",
    "        \n",
    "        assert q_preds.shape == (self.mb_size,)\n",
    "    \n",
    "        # Target Q net\n",
    "        with torch.no_grad():\n",
    "            nxt_tgt_net_vals = self.target_q(next_states) # shape:(m_batch, n_actions)\n",
    "            assert nxt_tgt_net_vals.shape == (self.mb_size, 2)\n",
    "            \n",
    "            # Get the max over columns and take only the values - not indices\n",
    "            max_tgt_net_vals = torch.max(nxt_tgt_net_vals, dim=1)[0] # shape: (m_batch, 1)\n",
    "#             max_tgt_net_vals.unsqueeze_(-1)\n",
    "            \n",
    "            assert max_tgt_net_vals.shape == (self.mb_size,), max_tgt_net_vals.shape # does this shape work for subtraction/addition?\n",
    "            assert max_tgt_net_vals.shape == rewards.shape, rewards.shape\n",
    "            assert max_tgt_net_vals.shape == dones.shape, dones.shape\n",
    "            \n",
    "            q_targets = rewards + self.gamma * (1 - dones) * max_tgt_net_vals # check\n",
    "            \n",
    "            assert q_preds.shape == q_targets.shape\n",
    "            print(q_preds.shape)\n",
    "        \n",
    "        # learn\n",
    "        self.optimiser.zero_grad()\n",
    "        loss = self.criterion(q_preds, q_targets) # maybe unsqueeze inputs here?\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    # Tested\n",
    "    def _eps_decay(self):\n",
    "        if self.eps_decay_rate != 1:\n",
    "            self.epsilon *= self.eps_decay_rate\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    # TODO: Test this works and there are no grads?\n",
    "    @torch.no_grad()\n",
    "    def _target_update(self):\n",
    "        self.target_q.load_state_dict(self.q.state_dict())\n",
    "        return True\n",
    "    \n",
    "    # TODO: Check this is complete and test it\n",
    "    def update(self):\n",
    "        \"\"\" \n",
    "        Wrapper function around the component updates\n",
    "        \"\"\"\n",
    "        self._eps_decay()\n",
    "        \n",
    "        self._target_timer += 1\n",
    "        if self._target_timer % self.target_update_freq == 0:\n",
    "            self._target_update()\n",
    "        \n",
    "        # If the agent has collected enough experience\n",
    "        if len(self.memory) > self.mb_size:\n",
    "            self._q_update()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # TODO: Comment this and test\n",
    "    def run_episode(self, render=False):\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        i = 0\n",
    "        \n",
    "        while i < self.step_lim:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _= self.env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if render: self.env.render()\n",
    "            \n",
    "            self.memory.store((state, action, reward, done, next_state))\n",
    "            self.update()\n",
    "                \n",
    "            if done: break\n",
    "            state = next_state\n",
    "            i += 1\n",
    "        return total_reward\n",
    "    \n",
    "    def train(self, num_epi, render=False):\n",
    "        rewards = [self.run_episode(render) for _ in range(num_epi)]\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(dqn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from typing import List, Tuple\n",
    "from deeprl.common.utils import get_gym_space_shape, net_gym_space_dims\n",
    "from torch.distributions import Categorical\n",
    "from deeprl.common.base import *\n",
    "from deeprl.common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVNAME = 'MountainCar-v0'\n",
    "env = gym.make(ENVNAME)\n",
    "env2 = gym.make('BipedalWalker-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = [\n",
    "    (nn.Linear, {\"in_features\":net_gym_space_dims(env.observation_space), \"out_features\":128}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\":128, \"out_features\": 64}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\":64, \"out_features\": net_gym_space_dims(env.action_space)})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Network(des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The value argument to log_prob must be a Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32m/home/akshil/deeprl/working_copy.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000007vscode-remote?line=2'>3</a>\u001b[0m policy \u001b[39m=\u001b[39m CategoricalPolicy(des)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m a \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mget_action(state)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m lp \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mget_log_prob(state, a)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m ent \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mget_entropy(state)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m \u001b[39massert\u001b[39;00m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(a\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy())\n",
      "\n",
      "File \u001b[0;32m~/deeprl/deeprl/common/base.py:42\u001b[0m, in \u001b[0;36mPolicy.get_log_prob\u001b[0;34m(self, states, actions)\u001b[0m\n",
      "\u001b[1;32m     <a href='file:///home/akshil/deeprl/deeprl/common/base.py?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_log_prob\u001b[39m(\u001b[39mself\u001b[39m, states, actions):\n",
      "\u001b[1;32m     <a href='file:///home/akshil/deeprl/deeprl/common/base.py?line=40'>41</a>\u001b[0m     prob_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(states)\n",
      "\u001b[0;32m---> <a href='file:///home/akshil/deeprl/deeprl/common/base.py?line=41'>42</a>\u001b[0m     log_prob \u001b[39m=\u001b[39m prob_dist\u001b[39m.\u001b[39;49mlog_prob(actions)\n",
      "\u001b[1;32m     <a href='file:///home/akshil/deeprl/deeprl/common/base.py?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m log_prob\n",
      "\n",
      "File \u001b[0;32m~/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py:117\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py?line=115'>116</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n",
      "\u001b[0;32m--> <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py?line=116'>117</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py?line=117'>118</a>\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/categorical.py?line=118'>119</a>\u001b[0m     value, log_pmf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n",
      "\n",
      "File \u001b[0;32m~/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py:263\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=248'>249</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=249'>250</a>\u001b[0m \u001b[39mArgument validation for distribution methods such as `log_prob`,\u001b[39;00m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=250'>251</a>\u001b[0m \u001b[39m`cdf` and `icdf`. The rightmost dimensions of a value to be\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=259'>260</a>\u001b[0m \u001b[39m        distribution's batch and event shapes.\u001b[39;00m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=260'>261</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=261'>262</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n",
      "\u001b[0;32m--> <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=262'>263</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe value argument to log_prob must be a Tensor\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=264'>265</a>\u001b[0m event_dim_start \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(value\u001b[39m.\u001b[39msize()) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_shape)\n",
      "\u001b[1;32m    <a href='file:///home/akshil/deeprl/rl_env/lib/python3.10/site-packages/torch/distributions/distribution.py?line=265'>266</a>\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39msize()[event_dim_start:] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_shape:\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = torch.from_numpy(state).unsqueeze(0)\n",
    "policy = CategoricalPolicy(des)\n",
    "a = policy.get_action(state)\n",
    "lp = policy.get_log_prob(state, a)\n",
    "ent = policy.get_entropy(state)\n",
    "\n",
    "assert env.action_space.contains(a.cpu().detach().squeeze().numpy())\n",
    "assert type(lp) == torch.Tensor\n",
    "assert lp.dtype == torch.float\n",
    "assert lp.requires_grad\n",
    "assert lp.shape == (1,)\n",
    "\n",
    "assert type(ent) == torch.Tensor\n",
    "assert ent.dtype == torch.float\n",
    "assert ent.requires_grad\n",
    "assert ent.shape == (1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "states = torch.stack([torch.from_numpy(state.astype(np.float32)).to('cpu') for _ in range(10)])\n",
    "policy = CategoricalPolicy(des)\n",
    "a = policy.get_action(states)\n",
    "lp = policy.get_log_prob(states, a)\n",
    "ent = policy.get_entropy(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10]) torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape, lp.shape, ent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13572\\2478511768.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2\u001b[0m policy_layers = [\n",
      "\u001b[0;32m      3\u001b[0m     (nn.Linear,\n",
      "\u001b[0;32m      4\u001b[0m         {\"in_features\": net_gym_space_dims(env.observation_space),\n",
      "\u001b[0;32m      5\u001b[0m         \"out_features\": 20}),\n",
      "\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "policy_layers = [\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": net_gym_space_dims(env.observation_space),\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 20,\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,{\"in_features\": 20, \"out_features\": net_gym_space_dims(env.action_space)}),\n",
    "]\n",
    "\n",
    "critic_layers = [\n",
    "    (nn.Linear, {\"in_features\": net_gym_space_dims(env.observation_space), \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear,\n",
    "        {\"in_features\": 20,\n",
    "        \"out_features\": 20}),\n",
    "    (nn.ReLU, {}),\n",
    "    (nn.Linear, {\"in_features\": 20, \"out_features\": 1}),\n",
    "]\n",
    "\n",
    "a2c_args = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"env\": env,\n",
    "    \"step_lim\": 200,\n",
    "    \"policy\": CategoricalPolicy(policy_layers),\n",
    "    \"policy_optimiser\": optim.Adam,\n",
    "    \"policy_lr\": 0.002,\n",
    "    \"critic\": Network(critic_layers),\n",
    "    \"critic_lr\": 0.002,\n",
    "    \"critic_optimiser\": optim.Adam,\n",
    "    \"critic_criterion\": nn.MSELoss(),\n",
    "    \"device\": \"cpu\",\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_train_passes\": 1,\n",
    "    \"lam\": 0.95,\n",
    "    \"num_eval_episodes\": 15\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing choose action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A2C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/home/akshil/deeprl/working_copy.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m agent \u001b[39m=\u001b[39m A2C(a2c_args)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m s \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/akshil/deeprl/working_copy.ipynb#ch0000011vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A2C' is not defined"
     ]
    }
   ],
   "source": [
    "agent = A2C(a2c_args)\n",
    "s = env.reset()\n",
    "for _ in range(200):\n",
    "    a = agent.choose_action(s)\n",
    "    assert env.action_space.contains(a)\n",
    "    assert lp.requires_grad\n",
    "    assert ent.requires_grad\n",
    "    s_, r, d, _ = env.step(a)\n",
    "    if d:\n",
    "        s = env.reset()\n",
    "    else:\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=A2C(a2c_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not TimeLimit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17624\\1358069809.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\akshil\\Documents\\deeprl\\deeprl\\algos\\a2c\\a2c.py\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, state)\u001b[0m\n",
      "\u001b[0;32m    280\u001b[0m             \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maction\u001b[0m \u001b[0mselected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    281\u001b[0m         \"\"\"\n",
      "\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not TimeLimit"
     ]
    }
   ],
   "source": [
    "agent.choose_action(agent.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_b = process_batch(out[1][1], 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9812],\n",
       "        [1.0230],\n",
       "        [0.9809],\n",
       "        [1.0228],\n",
       "        [1.0191],\n",
       "        [1.0182],\n",
       "        [1.0113],\n",
       "        [0.9876],\n",
       "        [1.0115],\n",
       "        [1.0076],\n",
       "        [0.9990],\n",
       "        [0.9986],\n",
       "        [1.0782],\n",
       "        [0.8529],\n",
       "        [1.0221],\n",
       "        [0.9806],\n",
       "        [0.9873],\n",
       "        [0.9725],\n",
       "        [1.0309],\n",
       "        [1.0163],\n",
       "        [1.0209],\n",
       "        [0.9812],\n",
       "        [0.9834],\n",
       "        [1.0193],\n",
       "        [1.0187],\n",
       "        [0.9832],\n",
       "        [0.9791],\n",
       "        [1.0234],\n",
       "        [0.9744],\n",
       "        [0.9574],\n",
       "        [0.9554],\n",
       "        [1.2706],\n",
       "        [0.8938],\n",
       "        [1.0224],\n",
       "        [0.9787],\n",
       "        [1.0213],\n",
       "        [0.9796],\n",
       "        [1.0207],\n",
       "        [0.9801],\n",
       "        [0.9847],\n",
       "        [0.9891],\n",
       "        [0.9829],\n",
       "        [1.0215],\n",
       "        [0.9835],\n",
       "        [0.9874],\n",
       "        [1.0175],\n",
       "        [0.9871],\n",
       "        [0.9780],\n",
       "        [0.9640],\n",
       "        [0.9618],\n",
       "        [1.0427],\n",
       "        [0.9605],\n",
       "        [0.9610],\n",
       "        [0.9630],\n",
       "        [0.9606],\n",
       "        [1.0382],\n",
       "        [1.3653],\n",
       "        [0.8581],\n",
       "        [1.0222],\n",
       "        [1.0167],\n",
       "        [0.9853],\n",
       "        [0.9807],\n",
       "        [0.9858],\n",
       "        [1.0175],\n",
       "        [0.9858],\n",
       "        [1.0171],\n",
       "        [0.9860],\n",
       "        [0.9770],\n",
       "        [0.9595],\n",
       "        [0.9593],\n",
       "        [0.9557],\n",
       "        [0.9573],\n",
       "        [0.9568],\n",
       "        [1.4109],\n",
       "        [0.8939],\n",
       "        [1.0268],\n",
       "        [1.0154],\n",
       "        [0.9839],\n",
       "        [0.9762],\n",
       "        [1.0249],\n",
       "        [1.0151],\n",
       "        [0.9842],\n",
       "        [0.9777],\n",
       "        [1.0232],\n",
       "        [1.0127],\n",
       "        [0.9868],\n",
       "        [1.0109],\n",
       "        [1.0032],\n",
       "        [1.0771],\n",
       "        [0.8907],\n",
       "        [0.9865],\n",
       "        [0.9826],\n",
       "        [0.9854],\n",
       "        [1.0180],\n",
       "        [0.9854],\n",
       "        [0.9791],\n",
       "        [0.9616],\n",
       "        [1.0404],\n",
       "        [0.9596],\n",
       "        [0.9589]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_td_deltas(batch=p_b, gamma=0.99, critic=agent.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "plt.title('A2C-GAE on CartPole')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Episodic Reward')\n",
    "plt.plot(out[-1])\n",
    "# plt.legend()\n",
    "\n",
    "plt.savefig('./data/a2c_gae_cartpole_learning_curve.PNG')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for agent number 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'A2C' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17000\\1184585802.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m     \u001b[1;31m# env.seed(i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'A2C' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "num_agents = 5\n",
    "num_epi = 200\n",
    "r = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    print(\"Running training for agent number {}\".format(i))\n",
    "    agent = A2C(a2c_args)\n",
    "        \n",
    "    # random.seed(i)\n",
    "    # np.random.seed(i)\n",
    "    # torch.manual_seed(i)\n",
    "    # env.seed(i)\n",
    "\n",
    "    r.append(agent.train(num_epi))\n",
    "\n",
    "out = np.array(r).mean(0)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.title('A2C on cartpole')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episodic Reward')\n",
    "plt.plot(out, label='rewards')\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('./data/a2c_cartpole.PNG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing mutability of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.vector.make(\"CartPole-v1\", 2, asynchronous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05743215 -0.5994613   0.09216133  1.0078765 ]\n",
      " [ 0.14062183  0.973484   -0.14432696 -1.535625  ]] [1 1] [1. 1.] [[-0.06942137 -0.40568253  0.11231886  0.7454989 ]\n",
      " [ 0.16009152  1.1700188  -0.17503946 -1.8696471 ]] [False False]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "a = [None for i in range(10)]\n",
    "b = [None for i in range(10)]\n",
    "c = []\n",
    "state = deepcopy(envs.observations)\n",
    "for i in range(10):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    if done.all(): \n",
    "        print(\"hi\")\n",
    "\n",
    "    if i == 9:\n",
    "        print(state, action, reward, next_state, done)\n",
    "\n",
    "    a[i] = to_torch(state, 'cpu')\n",
    "    b[i] = to_torch(next_state, 'cpu')\n",
    "    c.append(to_torch(action, 'cpu'))\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print((a[i]==b[i]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(batches)):\n",
    "    for j in range(len(batches[i][\"states\"])):\n",
    "        if (batches[i]['states'][j]==batches[i][\"next_states\"][j]).all():\n",
    "            print(i, j)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
