{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "### DQN on cartpole \n",
    "- [x] Read about DQN and plan classes\n",
    "- [ ] Test for dtypes, shapes, correct gradient changes, everything is a tensor, tests for making sure no_grad is on in the right points\n",
    "- [ ] Assert optimiser has zero grad just before it calculates the gradients\n",
    "- [ ] Make sure every input to a network is a tensor, and every input to a gym env is of right type and shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions/cheat sheet\n",
    "- When do i stop the gradients? Only have gradients when computing the loss for an update. \n",
    "    - Compute gradients for calculating things which have used the weights which need updating. \n",
    "    - I don't want requires_grad for environment outputs, only weights/params in the learnable nets\n",
    "- Do the dimensions of inputs to network have to include batch size, even if it is always 1?  I think so. Whenever an input goes into a network basically.\n",
    "- How do i pass a gradient through a distribution\n",
    "    - Using rdist\n",
    "- Convert everything that needs to go into a network to a tensor, float, device, has to have 0th dim as batch size when input for a net\n",
    "    - Dtype is float for everything since tensors are only used to go in the network. \n",
    "    - So the dtype only matters in the networks, and float is safe for passing through networks.\n",
    "- Use `.gather()` for getting the Q values as it is differentiable \n",
    "- Do i need to do a deepcopy of all transitions when sampling from buffer? \n",
    "    - No because I convert them to a tensor anyway which produces a new memory and reference\n",
    "- Only need to transfer to device when I when I create a new tensor or model that you want the parameters to be on cuda. \n",
    "    - So model has params (weights) so it is on cuda, most loss functions are not on cuda, any inputs and outputs to something on cuda are on cuda, so states etc, inputs etc have to be on cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces.discrete import Discrete\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict\n",
    "from gym import Space\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshil\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "ENVNAME = 'CartPole-v1'\n",
    "env = gym.make(ENVNAME)\n",
    "env2 = gym.make('BipedalWalker-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gym_space_shape(space):\n",
    "    if isinstance(space, Box):\n",
    "        return space.shape[0]\n",
    "    elif isinstance(space, Discrete):\n",
    "        return space.n\n",
    "    else:\n",
    "        raise TypeError(\"You haven't input a valid gym space. Here is your input: {}\".format(space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_torch_dtype(np_dtype):\n",
    "    numpy_to_torch_dtype = {\n",
    "        np.bool_       : torch.bool,\n",
    "        np.uint8      : torch.uint8,\n",
    "        np.int8       : torch.int8,\n",
    "        np.int16      : torch.int16,\n",
    "        np.int32      : torch.int32,\n",
    "        np.int64      : torch.int64,\n",
    "        np.float16    : torch.float16,\n",
    "        np.float32    : torch.float32,\n",
    "        np.float64    : torch.float64,\n",
    "        np.complex64  : torch.complex64,\n",
    "        np.complex128 : torch.complex128\n",
    "    }\n",
    "    return numpy_to_torch_dtype[np_dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, design: List):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "\n",
    "        # Define each layer according to arch.\n",
    "        for i in range(len(design)):\n",
    "            layer_type, params = design[i]\n",
    "            self.layers.append(layer_type(**params))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert type(x) == torch.Tensor\n",
    "        assert x.dim() > 1\n",
    "        assert x.dtype in [float, torch.float, torch.float64]\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"\n",
    "    Memeory does not convert what is given from the environment. It only provides storage and access.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, device):\n",
    "        self.max_len = max_len\n",
    "        self.buffer = self._reset_buffer()\n",
    "        self.device = device\n",
    "#         self.state_dtype = numpy_to_torch_dtype(env.observation_space.dtype)\n",
    "#         self.action_dtype = numpy_to_torch_dtype(env.action_space.dtype)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def _reset_buffer(self):\n",
    "        return deque(maxlen=self.max_len)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        returns an iterable(states, actions, rewards, dones, next_states)\n",
    "        \"\"\"\n",
    "        buffer_len = len(self.buffer)\n",
    "        # If there aren't enough samples then take the min\n",
    "        samples = random.sample(self.buffer, min([num_samples, buffer_len]))\n",
    "        \n",
    "        # separate lists for each part of transition\n",
    "        states, actions, rewards, dones, next_states = zip(*samples)\n",
    "        \n",
    "        # Convert to tensors and put on device\n",
    "        # calculate targets with target net \n",
    "        states = torch.tensor(states, dtype=torch.float, device=self.device) # shape:(mb_size, state_dim) using torch.float downgrades to float32 which i don't think matters...\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float, device=self.device) #shape: (mb_size, state_dim)\n",
    "        actions = torch.tensor(actions, dtype=torch.float, device=self.device) # shape:(mb_size, action_dim) used for indexing only \n",
    "        \n",
    "        rewards = torch.tensor([rewards], dtype=torch.float, device=self.device) # mb_size,added to output\n",
    "        dones = torch.tensor(dones, dtype=torch.int, device=self.device) # (mb_size, 1)\n",
    "        \n",
    "        return states, actions, rewards, dones, next_states\n",
    "            \n",
    "            \n",
    "    def store(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_memory_sample():\n",
    "    m = Memory(100, 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ENVNAME = 'CartPole-v1'\n",
    "    env = gym.make(ENVNAME)\n",
    "    \n",
    "    s = env.reset()\n",
    "    for i in range(50):\n",
    "        a = env.action_space.sample()\n",
    "        s_, r, d, _ = env.step(a)\n",
    "        t = [s, a, r, d, s_]\n",
    "        m.store(t)\n",
    "        if d: \n",
    "            s = env.reset()\n",
    "        \n",
    "    states, actions, rewards, dones, next_states = m.sample(120)\n",
    "    assert len(m) == 50\n",
    "    assert states.shape == (50, 4)\n",
    "test_memory_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVNAME = 'CartPole-v1'\n",
    "env = gym.make(ENVNAME)\n",
    "m = Memory(100, 'cuda')\n",
    "s = env.reset()\n",
    "for i in range(50):\n",
    "    a = env.action_space.sample()\n",
    "    s_, r, d, _ = env.step(a)\n",
    "    t = [s, a, r, d, s_]\n",
    "    m.store(t)\n",
    "    if d: \n",
    "        s = env.reset()\n",
    "\n",
    "states, actions, rewards, dones, next_states = m.sample(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = get_gym_space_shape(env.observation_space)\n",
    "output_dim = get_gym_space_shape(env.action_space)\n",
    "net_layers = [(nn.Linear, {\"in_features\": input_dim, \"out_features\": 16}), \n",
    "              (nn.ReLU, {}),\n",
    "              (nn.Linear, {\"in_features\": 16, \"out_features\": 4}),\n",
    "              (nn.ReLU, {}),\n",
    "              (nn.Linear, {\"in_features\": 4, \"out_features\": output_dim})]\n",
    "cartpole_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dqn_args = {'gamma'          : 0.9,\n",
    "            'epsilon'        : 0.3,\n",
    "            'eps_decay_rate' : 0.999,\n",
    "            'env'            : cartpole_env,\n",
    "            'step_lim'       : 200,\n",
    "            'mb_size'        : 32,\n",
    "            'net_design'     : net_layers,\n",
    "            'optimiser'      : optim.Adam,\n",
    "            'lr'             : 0.0001,\n",
    "            'polyak_w'       : 1.,\n",
    "            'memory_max_len' : 50000,\n",
    "            'device'         : 'cpu', #torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "            'criterion'      : nn.MSELoss(),\n",
    "            'target_update_freq' : 30            \n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, kwargs):\n",
    "        # algo params\n",
    "        self.gamma = kwargs['gamma']\n",
    "        self.lr = kwargs['lr']\n",
    "        self.epsilon = kwargs['epsilon']\n",
    "        self.eps_decay_rate = kwargs['eps_decay_rate']\n",
    "        self.mb_size = kwargs['mb_size']\n",
    "        self.polyak_w = kwargs['polyak_w']\n",
    "        \n",
    "        # env params\n",
    "        self.step_lim = kwargs['step_lim']\n",
    "        self.env = kwargs['env']\n",
    "        \n",
    "        self.device = kwargs['device']\n",
    "        self.memory = Memory(kwargs['memory_max_len'], self.device)\n",
    "        self.q = Network(kwargs['net_design']) # use for action selection (nograd) and learning(grad)\n",
    "        self.q.to(self.device)\n",
    "        self.target_q = deepcopy(self.q)\n",
    "        self.target_q.eval()\n",
    "        self.target_update_freq = kwargs['target_update_freq']\n",
    "        self._target_timer = 0\n",
    "        self.criterion = kwargs['criterion']\n",
    "        self.optimiser = kwargs['optimiser'](self.q.parameters(), self.lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([state], dtype=torch.float, device=self.device) # now passing through network, so should be float really\n",
    "            \n",
    "            self.q.eval()\n",
    "            # convert to numpy so you can do random choice of max vals\n",
    "            q_vals = self.q(state).cpu().detach().numpy() # (1, num_ac) do i need to detach given that i have already no_grad?\n",
    "            self.q.train()\n",
    "    \n",
    "            greedy_actions = np.where(q_vals == q_vals.max())[0] \n",
    "            action = np.random.choice(greedy_actions)\n",
    "        \n",
    "        assert self.env.action_space.contains(action)\n",
    "        return action\n",
    "                 \n",
    "    def _q_update(self):\n",
    "        states, actions, rewards, dones, next_states = self.memory.sample(self.mb_size)\n",
    "        rewards = rewards.squeeze(0)\n",
    "        \n",
    "        # Online Q net - track this for backprop\n",
    "        state_q_vals = self.q(states)\n",
    "        \n",
    "        # actions are converted to ints and used as indices, then the whole thing is squeezed to match targets shape for loss\n",
    "        q_preds = torch.gather(state_q_vals, -1, actions.long().unsqueeze(-1))\n",
    "        q_preds.squeeze_(-1)\n",
    "        \n",
    "        assert q_preds.shape == (self.mb_size,)\n",
    "    \n",
    "        # Target Q net\n",
    "        with torch.no_grad():\n",
    "            nxt_tgt_net_vals = self.target_q(next_states) # shape:(m_batch, n_actions)\n",
    "            assert nxt_tgt_net_vals.shape == (self.mb_size, 2)\n",
    "            \n",
    "            # Get the max over columns and take only the values - not indices\n",
    "            max_tgt_net_vals = torch.max(nxt_tgt_net_vals, dim=1)[0] # shape: (m_batch, 1)\n",
    "#             max_tgt_net_vals.unsqueeze_(-1)\n",
    "            \n",
    "            assert max_tgt_net_vals.shape == (self.mb_size,), max_tgt_net_vals.shape # does this shape work for subtraction/addition?\n",
    "            assert max_tgt_net_vals.shape == rewards.shape, rewards.shape\n",
    "            assert max_tgt_net_vals.shape == dones.shape, dones.shape\n",
    "            \n",
    "            q_targets = rewards + self.gamma * (1 - dones) * max_tgt_net_vals # check\n",
    "            \n",
    "            assert q_preds.shape == q_targets.shape\n",
    "            print(q_preds.shape)\n",
    "        \n",
    "        # learn\n",
    "        self.optimiser.zero_grad()\n",
    "        loss = self.criterion(q_preds, q_targets) # maybe unsqueeze inputs here?\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    # Tested\n",
    "    def _eps_decay(self):\n",
    "        if self.eps_decay_rate != 1:\n",
    "            self.epsilon *= self.eps_decay_rate\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    # TODO: Test this works and there are no grads?\n",
    "    @torch.no_grad()\n",
    "    def _target_update(self):\n",
    "        self.target_q.load_state_dict(self.q.state_dict())\n",
    "        return True\n",
    "    \n",
    "    # TODO: Check this is complete and test it\n",
    "    def update(self):\n",
    "        \"\"\" \n",
    "        Wrapper function around the component updates\n",
    "        \"\"\"\n",
    "        self._eps_decay()\n",
    "        \n",
    "        self._target_timer += 1\n",
    "        if self._target_timer % self.target_update_freq == 0:\n",
    "            self._target_update()\n",
    "        \n",
    "        # If the agent has collected enough experience\n",
    "        if len(self.memory) > self.mb_size:\n",
    "            self._q_update()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # TODO: Comment this and test\n",
    "    def run_episode(self, render=False):\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        i = 0\n",
    "        \n",
    "        while i < self.step_lim:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _= self.env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if render: self.env.render()\n",
    "            \n",
    "            self.memory.store((state, action, reward, done, next_state))\n",
    "            self.update()\n",
    "                \n",
    "            if done: break\n",
    "            state = next_state\n",
    "            i += 1\n",
    "        return total_reward\n",
    "    \n",
    "    def train(self, num_epi, render=False):\n",
    "        rewards = [self.run_episode(render) for _ in range(num_epi)]\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(dqn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5, requires_grad=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[12.0, 14.0, 14.0, 11.0, 9.0, 10.0, 11.0, 9.0, 14.0, 11.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
