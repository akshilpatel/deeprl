
Running training for agent number 0
C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py:230: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:210.)
  state = torch.tensor([state], dtype=torch.float, device=self.device)
Episode 0, Reward 37.0
Episode 10, Reward 67.0
Episode 20, Reward 10.0
Episode 30, Reward 11.0
Episode 40, Reward 10.0
Episode 50, Reward 9.0
Episode 60, Reward 10.0
Episode 70, Reward 20.0
Episode 80, Reward 12.0
Episode 90, Reward 16.0
Episode 100, Reward 11.0
Episode 110, Reward 9.0
Episode 120, Reward 13.0
Episode 130, Reward 18.0
Episode 140, Reward 10.0
Episode 150, Reward 8.0
Episode 160, Reward 19.0
Episode 170, Reward 70.0
Episode 180, Reward 19.0
Episode 190, Reward 50.0
Running training for agent number 1
Episode 0, Reward 77.0
Episode 10, Reward 44.0
Episode 20, Reward 45.0
Episode 30, Reward 60.0
Episode 40, Reward 42.0
Episode 50, Reward 92.0
Episode 60, Reward 101.0
Episode 70, Reward 129.0
Traceback (most recent call last):
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 313, in <module>
    r.append(agent.train(num_epi))
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 60, in train
    total_rewards[i] = self.run_episode(render=render)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 88, in run_episode
    losses = self.update(state, reward, next_state, done, log_prob, entropy)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 132, in update
    critic_loss = self.update_critic(td_target, state)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 183, in update_critic
    self.critic_optimiser.step()
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\_functional.py", line 97, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt