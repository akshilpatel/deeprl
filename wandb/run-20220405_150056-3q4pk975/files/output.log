
Running training for agent number 0
C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py:230: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\torch\csrc\utils\tensor_new.cpp:210.)
  state = torch.tensor([state], dtype=torch.float, device=self.device)
Episode 0, Reward 17.0
Episode 10, Reward 13.0
Episode 20, Reward 10.0
Episode 30, Reward 9.0
Episode 40, Reward 8.0
Episode 50, Reward 15.0
Episode 60, Reward 11.0
Episode 70, Reward 9.0
Episode 80, Reward 11.0
Episode 90, Reward 15.0
Episode 100, Reward 38.0
Episode 110, Reward 9.0
Episode 120, Reward 10.0
Episode 130, Reward 14.0
Episode 140, Reward 12.0
Episode 150, Reward 48.0
Episode 160, Reward 61.0
Episode 170, Reward 200.0
Episode 180, Reward 45.0
Episode 190, Reward 142.0
Running training for agent number 1
Episode 0, Reward 110.0
Episode 10, Reward 24.0
Episode 20, Reward 103.0
Episode 30, Reward 41.0
Episode 40, Reward 126.0
Episode 50, Reward 90.0
Episode 60, Reward 90.0
Episode 70, Reward 119.0
Episode 80, Reward 200.0
Episode 90, Reward 200.0
Episode 100, Reward 152.0
Episode 110, Reward 18.0
Episode 120, Reward 14.0
Episode 130, Reward 43.0
Episode 140, Reward 115.0
Episode 150, Reward 116.0
Episode 160, Reward 200.0
Episode 170, Reward 200.0
Episode 180, Reward 200.0
Episode 190, Reward 9.0
Running training for agent number 2
Episode 0, Reward 13.0
Episode 10, Reward 12.0
Episode 20, Reward 12.0
Episode 30, Reward 16.0
Episode 40, Reward 157.0
Traceback (most recent call last):
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 313, in <module>
    r.append(agent.train(num_epi))
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 60, in train
    total_rewards[i] = self.run_episode(render=render)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 88, in run_episode
    losses = self.update(state, reward, next_state, done, log_prob, entropy)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 132, in update
    critic_loss = self.update_critic(td_target, state)
  File "C:\Users\akshil\Desktop\Extracurricular\coding_practice\deeprl\deeprl\algos\a2c\a2c.py", line 183, in update_critic
    self.critic_optimiser.step()
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\adam.py", line 141, in step
    F.adam(params_with_grad,
  File "C:\Users\akshil\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\optim\_functional.py", line 98, in adam
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)
KeyboardInterrupt